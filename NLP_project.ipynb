{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import gridworld as W                       # basic grid-world MDPs\n",
        "import trajectory as T                      # trajectory generation\n",
        "import optimizer as O                       # stochastic gradient descent optimizer\n",
        "import solver as S                          # MDP solver (value-iteration)\n",
        "import plot as P\n",
        "\n",
        "from maxent import irl"
      ],
      "metadata": {
        "id": "WoZ5ca3U6aAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "5d89f6a2-2cc7-4d82-a71e-fee7e1900615"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-65c4d2c82ed1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgridworld\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mW\u001b[0m                       \u001b[0;31m# basic grid-world MDPs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrajectory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m                      \u001b[0;31m# trajectory generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mO\u001b[0m                       \u001b[0;31m# stochastic gradient descent optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gridworld'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_state(word):\n",
        "    verb_positive_words = ['bloom', 'soar', 'sparkle', 'thrive', 'illuminate']\n",
        "    verb_negative_words = ['complain', 'argue', 'blame', 'grumble', 'snarl']\n",
        "    verb_inactive_words = ['rest', 'pause', 'hover', 'laze', 'bask']\n",
        "    verb_active_words = ['smash', 'grab', 'push', 'shout', 'storm']\n",
        "    adv_time_words = ['suddenly', 'gradually', 'eventually', 'shortly', 'instantly']\n",
        "    adv_place_words = ['here', 'there', 'everywhere', 'nowhere', 'somewhere']\n",
        "    adv_interrogative_words = ['curiously', 'wonderingly', 'questioningly', 'doubtfully', 'pensively']\n",
        "    adj_descriptive_words = ['bright', 'calm', 'colorful', 'peaceful', 'radiant']\n",
        "    adj_quantitative_words = ['little', 'few', 'some', 'many', 'much']\n",
        "    conj_uncertain_words = ['maybe', 'possibly', 'likely', 'arguably', 'presumably']\n",
        "    conj_certain_words = ['definitely', 'surely', 'clearly', 'obviously', 'undoubtedly']\n",
        "    conj_explanatory_words = ['because', 'since', 'as', 'so', 'for']\n",
        "    nouns_elaborated_words = ['adventure', 'journey', 'discovery', 'odyssey', 'quest', 'serenity', 'jubilee', 'harmony', 'oasis', 'beacon']\n",
        "    nouns_basic_words = ['car', 'house', 'job', 'money', 'phone', 'discord', 'blight', 'accident', 'damage', 'pain']\n",
        "    first_person_pronouns = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
        "    second_person_pronouns = ['you', 'your', 'yours']\n",
        "    third_person_pronouns = ['he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs']\n",
        "    prepositions = ['in', 'on', 'under', 'over', 'with', 'without']\n",
        "    determiners = ['the', 'a', 'an', 'this', 'that', 'these', 'those']\n",
        "\n",
        "    word_lower = word.lower()\n",
        "\n",
        "    if word_lower in verb_positive_words:\n",
        "        return 'Verb positive'\n",
        "    elif word_lower in verb_negative_words:\n",
        "        return 'Verb negative'\n",
        "    elif word_lower in verb_inactive_words:\n",
        "        return 'Verb inactive'\n",
        "    elif word_lower in verb_active_words:\n",
        "        return 'Verb active'\n",
        "    elif word_lower in adv_time_words:\n",
        "        return 'Adv Time'\n",
        "    elif word_lower in adv_place_words:\n",
        "        return 'Adv Place'\n",
        "    elif word_lower in adv_interrogative_words:\n",
        "        return 'Adv Interrogative'\n",
        "    elif word_lower in adj_descriptive_words:\n",
        "        return 'Adj Descriptive'\n",
        "    elif word_lower in adj_quantitative_words:\n",
        "        return 'Adj Quantitative'\n",
        "    elif word_lower in conj_uncertain_words:\n",
        "        return 'Conj uncertain'\n",
        "    elif word_lower in conj_certain_words:\n",
        "        return 'Conj certain'\n",
        "    elif word_lower in conj_explanatory_words:\n",
        "        return 'Conj explanatory'\n",
        "    elif word_lower in nouns_elaborated_words:\n",
        "        return 'Noun educated'\n",
        "    elif word_lower in nouns_basic_words:\n",
        "        return 'Noun basic'\n",
        "    elif word_lower in first_person_pronouns:\n",
        "        return 'P first person'\n",
        "    elif word_lower in second_person_pronouns:\n",
        "        return 'P second person'\n",
        "    elif word_lower in third_person_pronouns:\n",
        "        return 'P third person'\n",
        "    elif word_lower in prepositions:\n",
        "        return 'Preposition'\n",
        "    elif word_lower in determiners:\n",
        "        return 'Det'\n",
        "    else:\n",
        "        assert 1+1 == 5\n",
        "        return 'Unknown'\n",
        "\n",
        "# Example usage:\n",
        "word = 'ThRIve'\n",
        "result = word_to_state(word)\n",
        "print(f\"The word '{word}' corresponds to the state: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzg0oC0ovuej",
        "outputId": "78c2996f-4412-4037-da48-d137d412a577"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'ThRIve' corresponds to the state: Verb positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = [\n",
        "    'Verb positive', 'Verb negative', 'Verb active', 'Verb inactive',\n",
        "    'Adv Time', 'Adv Place', 'Adv Interrogative',\n",
        "    'Adj Descriptive', 'Adj Quantitative',\n",
        "    'Conj certain', 'Conj uncertain', 'Conj defensive', 'Conj explanatory',\n",
        "    'P first person', 'P second person', 'P third person',\n",
        "    'Noun basic', 'Noun educated',\n",
        "    'Preposition',\n",
        "    'Det'\n",
        "    ]\n",
        "\n",
        "\n",
        "states_one_hot = np.zeros((len(states), len(states)))\n",
        "\n",
        "one_hot_encoding_dic = {state: [0] * len(states) for state in states}\n",
        "\n",
        "# Set the corresponding index to 1 for each state\n",
        "for i, state in enumerate(states):\n",
        "    one_hot_encoding_dic[state][i] = 1\n",
        "    states_one_hot[i, i] = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "CGICH-Dmn2qZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states_one_hot[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7Y5XvrGo6S6",
        "outputId": "85b89c8c-922a-4f80-f11c-092c2306aa25"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding_dic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-kV2qgdp2Ae",
        "outputId": "7b01b174-9ba1-4b3e-d481-6aefb2e96340"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Verb positive': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'Verb negative': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'Verb active': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'Verb inactive': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'Adv Time': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'Adv Place': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'Adv Interrogative': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'Adj Descriptive': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'Adj Quantitative': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'Conj certain': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'Conj uncertain': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'Conj defensive': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'Conj explanatory': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'P first person': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'P second person': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'P third person': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'Noun basic': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              " 'Noun educated': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              " 'Preposition': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              " 'Det': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rsv_ZsTp18k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vukAUUfNp1y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVoFbrQin2oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7mDhU3zn2l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5QJiC_is0BO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Corpus provided by the user\n",
        "corpus = ['I love basketball',\n",
        "          'I love football',\n",
        "          'I love soccer',\n",
        "          'I love tennis',\n",
        "          'I love volleyball',\n",
        "          'I love cake',\n",
        "          'I love cookies',\n",
        "          'I love pizza',\n",
        "          'I love chocolate',\n",
        "          'I love candies',\n",
        "          ]\n",
        "\n",
        "corpus_1 = ['I love basketball',\n",
        "          'I love football',\n",
        "          'I love soccer',\n",
        "          'I love tennis',\n",
        "          'I love volleyball',]\n",
        "\n",
        "corpus_2 = ['I love cake',\n",
        "          'I love cookies',\n",
        "          'I love pizza',\n",
        "          'I love chocolate',\n",
        "          'I love candies',\n",
        "          ]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences and create a set of unique words\n",
        "unique_words = set(word for sentence in corpus for word in sentence.split())\n",
        "\n",
        "# Sort the unique words to have consistent order\n",
        "sorted_unique_words = sorted(list(unique_words))\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Reshape and fit transform the sorted unique words\n",
        "one_hot_encoded = encoder.fit_transform(np.array(sorted_unique_words).reshape(-1, 1))\n",
        "\n",
        "# Map each unique word to its one-hot encoded vector\n",
        "word_to_one_hot = dict(zip(sorted_unique_words, one_hot_encoded))\n",
        "\n",
        "# Display the one-hot encoded vectors\n",
        "for word, one_hot_vector in word_to_one_hot.items():\n",
        "    print(f\"Word: {word}, One-hot vector: {one_hot_vector}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omBsZgVntvEY",
        "outputId": "2b0becab-41dd-4cd1-c7d4-5e75754cc9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: I, One-hot vector: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: basketball, One-hot vector: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: cake, One-hot vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: candies, One-hot vector: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: chocolate, One-hot vector: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: cookies, One-hot vector: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Word: football, One-hot vector: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Word: love, One-hot vector: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Word: pizza, One-hot vector: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Word: soccer, One-hot vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Word: tennis, One-hot vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Word: volleyball, One-hot vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsri46jUtv1-",
        "outputId": "326047cc-7d8d-464d-99af-8fe8897f666e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_one_hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3HjjQL2tvxp",
        "outputId": "b921e53a-e9bd-46d9-9744-315640841fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'I': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'basketball': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cake': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'candies': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'chocolate': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cookies': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
              " 'football': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
              " 'love': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
              " 'pizza': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
              " 'soccer': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
              " 'tennis': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
              " 'volleyball': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the 3D matrix of zeros\n",
        "transition = np.zeros((12, 12, 12))\n",
        "\n",
        "# Iterate through the second dimension to set the ith column to ones\n",
        "for i in range(12):\n",
        "    transition[:, i, i] = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "-X3Te1gjtvrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D1 = []\n",
        "\n",
        "\n",
        "for sentence  in corpus_1:\n",
        "  episode = []\n",
        "  sentence = sentence.split()\n",
        "  for i in range(len(sentence) - 1):\n",
        "\n",
        "    s = np.argmax(word_to_one_hot[sentence[i]])\n",
        "    s_next = np.argmax(word_to_one_hot[sentence[i + 1]])\n",
        "    episode.append((s, s_next, s_next))\n",
        "\n",
        "  D1.append(T.Trajectory(episode))"
      ],
      "metadata": {
        "id": "UaUgj0xJtvvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5yRB5Hw8dUt",
        "outputId": "faaf0bf2-73d5-4755-c790-257f0c332aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p38cWDPZtvtX",
        "outputId": "07945c78-f2c8-47e3-9848-b7748998ac33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Trajectory([(0, 7, 7), (7, 1, 1)]),\n",
              " Trajectory([(0, 7, 7), (7, 6, 6)]),\n",
              " Trajectory([(0, 7, 7), (7, 9, 9)]),\n",
              " Trajectory([(0, 7, 7), (7, 10, 10)]),\n",
              " Trajectory([(0, 7, 7), (7, 11, 11)])]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "terminal1 = []\n",
        "\n",
        "for traj in D1:\n",
        "  terminal1.append(traj._t[-1][-1])\n",
        "\n",
        "terminal1 = list(set(terminal1))"
      ],
      "metadata": {
        "id": "qYcwKHLU8-Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terminal1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKtRXpVB9UAp",
        "outputId": "93977f66-be40-47ca-9ee6-c7d332a914d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 6, 9, 10, 11]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   initialize parameters with constant\n",
        "init = O.Constant(1.0)\n",
        "\n",
        "# choose our optimization strategy:\n",
        "#   we select exponentiated stochastic gradient descent with linear learning-rate decay\n",
        "optim = O.ExpSga(lr=O.linear_decay(lr0=0.2))\n",
        "\n",
        "# Computing the R function through inverse reinforcement learning\n",
        "reward_maxent1 = irl(transition, one_hot_encoded, terminal1, D1, optim, init)"
      ],
      "metadata": {
        "id": "L7jBmoGW38iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_maxent1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BgkjAJ239RM",
        "outputId": "8dee48e9-cd28-4c79-cc19-41534c9e219f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.7648935 , 1.0000095 , 0.7648935 , 0.7648935 , 0.7648935 ,\n",
              "       0.7648935 , 1.0000095 , 2.34831335, 0.7648935 , 1.0000095 ,\n",
              "       1.0000095 , 1.0000095 ])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_one_hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT0yzVSs39OW",
        "outputId": "4f9a085d-e7e1-464c-fa60-7e408cb71975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'I': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'basketball': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cake': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'candies': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'chocolate': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cookies': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
              " 'football': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
              " 'love': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
              " 'pizza': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
              " 'soccer': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
              " 'tennis': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
              " 'volleyball': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D2 = []\n",
        "\n",
        "\n",
        "for sentence  in corpus_2:\n",
        "  episode = []\n",
        "  sentence = sentence.split()\n",
        "  for i in range(len(sentence) - 1):\n",
        "\n",
        "    s = np.argmax(word_to_one_hot[sentence[i]])\n",
        "    s_next = np.argmax(word_to_one_hot[sentence[i + 1]])\n",
        "    episode.append((s, s_next, s_next))\n",
        "\n",
        "  D2.append(T.Trajectory(episode))"
      ],
      "metadata": {
        "id": "s2pF6crI-KII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terminal2 = []\n",
        "\n",
        "for traj in D2:\n",
        "  terminal2.append(traj._t[-1][-1])\n",
        "\n",
        "terminal2 = list(set(terminal2))"
      ],
      "metadata": {
        "id": "bS6L_puN-jn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   initialize parameters with constant\n",
        "init = O.Constant(1.0)\n",
        "\n",
        "# choose our optimization strategy:\n",
        "#   we select exponentiated stochastic gradient descent with linear learning-rate decay\n",
        "optim = O.ExpSga(lr=O.linear_decay(lr0=0.2))\n",
        "\n",
        "# Computing the R function through inverse reinforcement learning\n",
        "reward_maxent2 = irl(transition, one_hot_encoded, terminal2, D2, optim, init)"
      ],
      "metadata": {
        "id": "wzBxDhVS39JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_maxent2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms03nFgx-AN4",
        "outputId": "66171eab-94be-46c3-e7a7-b8476b165dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.7648935 , 0.7648935 , 1.0000095 , 1.0000095 , 1.0000095 ,\n",
              "       1.0000095 , 0.7648935 , 2.34831335, 1.0000095 , 0.7648935 ,\n",
              "       0.7648935 , 0.7648935 ])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_one_hot"
      ],
      "metadata": {
        "id": "ai52_euM-02X",
        "outputId": "71f499d4-b5b9-4f72-ae01-deebd727abc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'I': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'basketball': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cake': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'candies': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'chocolate': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " 'cookies': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
              " 'football': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
              " 'love': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
              " 'pizza': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
              " 'soccer': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
              " 'tennis': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
              " 'volleyball': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    }
  ]
}